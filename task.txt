С помощью связки библиотек requests+beautifulsoup4 (https://requests.readthedocs.io/en/latest/, https://beautiful-soup-4.readthedocs.io/en/latest/) спарсить сайт (у вас они будут разные, каждый парсит свой сайт):

@Danyaloveneeprk - https://rg.ru/,
@fgfgfgfgfgfgfgk - https://bezformata.com/,
@vgoloveopilki_da_da_da - https://uralpolit.ru/

Парсим начиная с главной страницы (парсить будем дату, заголовки и текст новостей), запускать парсер кадждый час и собирать новые новости.
В первый прогон собрать новости за последние 24 часа, далее уже только новые (проверять, что такой ссылки нет в бд). Все данные записывать в локальную базу (Postgresql) с уникальными id.
Самостоятельно создать бд и таблицу для этого (локально, но с добавлением в проект создания базы и таблицы, если они не существуют)

Работа с базой через sqlalchemy - ORM на питоне. (https://www.sqlalchemy.org/)
Прикрутить апишку (на fastapi - https://fastapi.tiangolo.com/), добавить туда get-запрос с получением новости из базы по параметру id (path-параметр).

Добавить requirements.txt
Добавить логгирование с помощью либы logging
Добавить обработку ошибок
Еще желательно сделать минимальную структуру проекта (парсер в отдельном файле, работа с бд в отдельном, апишка в отдельном, соединять и запускать это всё дело в main)
Завернуть проект в Docker
Залить код в личный гитхаб и скинуть ссылку после того, как всё сделаете
